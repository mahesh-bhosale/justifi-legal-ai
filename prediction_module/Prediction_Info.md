âš–ï¸ JustiFi â€“ AI-Powered Legal Decision Support System

JustiFi is an AI-driven legal intelligence platform designed to assist legal professionals by predicting case outcomes, retrieving similar legal cases, and providing explainability for decisions using state-of-the-art InLegalBERT and NLP techniques.

This project is developed as a final-year major project and focuses on real-world scalability, explainability, and deployment readiness.

ğŸš€ Key Features
ğŸ”¹ 1. Legal Outcome Prediction

Predicts ACCEPT / REJECT for legal cases

Powered by InLegalBERT

Uses chunk-based inference to handle long legal documents (beyond 512 tokens)

ğŸ”¹ 2. Chunk-Based Document Processing

Splits long judgments into overlapping chunks

Aggregates predictions using probability averaging

Ensures full-document understanding

ğŸ”¹ 3. Similar Case Retrieval

Finds legally similar past cases

Uses sentence embeddings + cosine similarity

Works on entire documents, not just first 512 tokens

ğŸ”¹ 4. Explainability (Why This Prediction?)

Shows:

Confidence score

Number of chunks considered

Similar cases that influenced the decision

Designed for legal transparency

ğŸ”¹ 5. PDF Upload & Processing

Upload any legal PDF

Extracts text automatically

Runs prediction + similarity search

ğŸ”¹ 6. API-Based Architecture

FastAPI backend

Modular, production-ready structure

Can be integrated with frontend or mobile apps

ğŸ§  Model & Dataset
ğŸ”¸ Model Used

InLegalBERT (Legal-domain BERT model)

Fine-tuned on Indian legal judgments

Binary classification: ACCEPT (1) / REJECT (0)

ğŸ”¸ Dataset

Source: CJPE / NyayaAnumana legal datasets

Courts covered:

Supreme Court

High Courts

Tribunals

Dataset size: 300K+ legal cases

Labels:

0 â†’ Rejected

1 â†’ Accepted

âš ï¸ Datasets are NOT included in this repository due to size and licensing constraints.

ğŸ“‚ Project Structure
JUSTIFI-LEGAL-AI/
â”‚
â”œâ”€â”€ backend/                 # Backend services (FastAPI)
â”‚
â”œâ”€â”€ datasets/                # (Ignored) Raw legal datasets
â”‚
â”œâ”€â”€ ml_model/
â”‚   â”œâ”€â”€ routes/              # API routes
â”‚   â”œâ”€â”€ utils/               # PDF, text utilities
â”‚   â”œâ”€â”€ model_loader.py      # Loads trained model
â”‚   â”œâ”€â”€ summarizer.py
â”‚   â”œâ”€â”€ run_server.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ prediction_module/
â”‚   â”œâ”€â”€ inlegalbert_final/   # (Ignored) Trained model
â”‚   â”œâ”€â”€ bert_prediction.ipynb
â”‚   â”œâ”€â”€ inlegalbert_inference.ipynb
â”‚   â”œâ”€â”€ data_processing.ipynb
â”‚   â””â”€â”€ prediction.py        # FastAPI inference API
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ğŸ› ï¸ Tech Stack
Category	Tools
Language	Python 3.10
Model	InLegalBERT
NLP	HuggingFace Transformers
ML	PyTorch
API	FastAPI
PDF	pdfplumber
Similarity	Sentence Transformers
Deployment	Uvicorn
âš™ï¸ Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/justifi-legal-ai.git
cd justifi-legal-ai

2ï¸âƒ£ Create Virtual Environment
python -m venv legal_env
legal_env\Scripts\activate   # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

â–¶ï¸ Running the API Server
Start Prediction API
uvicorn prediction:app --reload


API will run at:

http://127.0.0.1:8000


Swagger UI:

http://127.0.0.1:8000/docs

ğŸ“„ API Endpoints
ğŸ”¹ Predict Case Outcome (PDF)

POST /predict/pdf

Response:

{
  "prediction": "ACCEPT",
  "confidence": 0.73,
  "num_chunks": 8
}

ğŸ”¹ Predict Case Outcome (Text)

POST /predict/text

ğŸ”¹ Similar Case Retrieval

POST /similar_cases

Returns:

Top similar judgments

Similarity score

Reason for similarity

ğŸ“Š Model Performance (Chunk-Based)
Metric	Score
Accuracy	0.735
Precision	0.70
Recall	0.76
F1-score	0.73

Chunk-based inference significantly improves performance over 512-token limitation.

ğŸ” Explainability Strategy

JustiFi provides explainability by:

Chunk-level predictions

Confidence aggregation

Similar case evidence

Transparent scoring

This makes predictions interpretable and trustworthy for legal use.

â— Why Chunking?

Legal judgments are very long (10kâ€“30k words).

BERT limit = 512 tokens

âœ” Chunking ensures:

No loss of legal reasoning

Full-document context

Better real-world accuracy

ğŸ“Œ Why Datasets & Models Are Ignored

Extremely large size (GBs)

Licensing restrictions

Can be regenerated via scripts

All training and preprocessing code is included.

ğŸ”® Future Enhancements

Multilingual legal support

Advanced legal reasoning graphs

RAG-based legal chatbot

Timeline extraction

Citation analysis

ğŸ‘¨â€ğŸ“ Academic Note

This project is developed as a final-year engineering major project focusing on:

Applied AI

Legal NLP

Explainable ML

Production deployment


â­ If You Like This Project

Give it a â­ on GitHub â€” it really helps!